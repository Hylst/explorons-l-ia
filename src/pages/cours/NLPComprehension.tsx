import React from 'react';
import Hero from '@/components/Hero';
import CourseHeader from '@/components/courses/CourseHeader';
import CourseModule from '@/components/courses/CourseModule';
import CourseConclusion from '@/components/courses/CourseConclusion';
import ZoomOn from '@/components/courses/ZoomOn';
import DidYouKnow from '@/components/courses/DidYouKnow';
import TechnicalTooltip from '@/components/courses/TechnicalTooltip';
import AnalogyBox from '@/components/courses/AnalogyBox';
import ConceptCard from '@/components/courses/nlp/ConceptCard';
import ArchitectureComparison from '@/components/courses/nlp/ArchitectureComparison';
import TransformerVisualizer from '@/components/courses/nlp/TransformerVisualizer';
import InteractiveSection from '@/components/courses/nlp/InteractiveSection';
import PracticalExample from '@/components/courses/nlp/PracticalExample';
import HistoricalComparison from '@/components/courses/nlp/HistoricalComparison';
import LLMStatsShowcase from '@/components/courses/nlp/LLMStatsShowcase';
import FutureApplications from '@/components/courses/nlp/FutureApplications';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { Badge } from '@/components/ui/badge';
import { 
  MessageSquare, 
  Brain, 
  Layers, 
  Zap, 
  Book, 
  Languages,
  Cpu,
  Network,
  Target,
  FileText,
  Bot,
  Lightbulb,
  Scissors,
  Hash,
  Calculator,
  Eye,
  Shuffle,
  TrendingUp,
  Users,
  Globe,
  Shield,
  AlertTriangle,
  Sparkles,
  Code,
  Database,
  ChartBar,
  Clock,
  Rocket,
  History,
  Compass
} from 'lucide-react';

const NLPComprehension = () => {
  const headerProps = {
    title: "Comprendre le Traitement du Langage Naturel (NLP) et les LLM",
    subtitle: "Du texte brut aux mod√®les de langage les plus avanc√©s : un voyage au c≈ìur de la compr√©hension artificielle",
    author: "Geoffroy Streit",
    authorDescription: "Educateur en intelligence artificielle",
    duration: "6-8 heures",
    level: "Interm√©diaire √† Avanc√©",
    audience: "Curieux de technologie, d√©veloppeurs, √©tudiants",
    tags: ["NLP", "LLM", "Transformers", "Deep Learning", "IA G√©n√©rative", "2024"]
  };

  // Donn√©es enrichies pour les modules avec explications d√©taill√©es
  const module1Data = [
    {
      icon: <Languages className="h-5 w-5 text-primary" />,
      title: "Le d√©fi du langage",
      items: [
        "Pourquoi le langage humain est-il si complexe ?",
        "Ambigu√Øt√©, contexte et nuances culturelles",
        "Diff√©rences entre communication humaine et machine",
        "Histoire des premi√®res tentatives automatiques",
        "√âchecs et le√ßons des syst√®mes symboliques",
        "L'√©volution vers l'approche statistique"
      ],
      expandedExplanations: [
        {
          title: "Pourquoi le langage humain est-il si complexe ?",
          content: "Le langage humain n'est pas qu'un simple syst√®me de symboles. Il porte en lui des mill√©naires d'√©volution culturelle et cognitive. Contrairement aux langages de programmation qui sont pr√©cis et sans ambigu√Øt√©, le langage naturel regorge d'implicites, de r√©f√©rences contextuelles et de sous-entendus. Chaque phrase peut avoir plusieurs interpr√©tations selon le contexte, l'intonation, ou m√™me le silence qui l'entoure. Cette richesse qui fait la beaut√© du langage humain constitue un d√©fi colossal pour les machines."
        },
        {
          title: "Ambigu√Øt√©, contexte et nuances culturelles",
          content: "L'ambigu√Øt√© est omnipr√©sente dans le langage. Le mot 'avocat' peut d√©signer un fruit ou un juriste. La phrase 'Il a vu l'homme avec les jumelles' peut signifier que l'homme avait des jumelles ou que la personne utilisait des jumelles pour le voir. Les nuances culturelles ajoutent une complexit√© suppl√©mentaire : l'ironie, le sarcasme, les r√©f√©rences culturelles partag√©es, les expressions idiomatiques qui n'ont aucun sens litt√©ral. Ces subtilit√©s n√©cessitent une connaissance du monde et des conventions sociales."
        },
        {
          title: "Diff√©rences entre communication humaine et machine",
          content: "Les humains communiquent avec tout leur √™tre : gestes, expressions faciales, ton de voix, contexte situationnel. Nous inf√©rons constamment les intentions, comblons les blancs, et adaptons notre interpr√©tation en temps r√©el. Nous utilisons notre exp√©rience du monde, nos √©motions, et notre th√©orie de l'esprit pour comprendre ce que l'autre veut vraiment dire. Les machines, traditionnellement, ont √©t√© limit√©es √† des r√®gles explicites et des correspondances directes, incapables de cette flexibilit√© naturelle."
        },
        {
          title: "Histoire des premi√®res tentatives automatiques",
          content: "D√®s les ann√©es 1950, les pionniers de l'informatique r√™vaient de machines capables de comprendre et traduire les langues. Les premiers syst√®mes √©taient bas√©s sur des dictionnaires bilingues et des r√®gles grammaticales simples. L'optimisme √©tait √©norme : on pensait r√©soudre la traduction automatique en quelques ann√©es. La d√©monstration Georgetown-IBM de 1954 traduisait 60 phrases du russe vers l'anglais, cr√©ant un enthousiasme d√©bordant et des pr√©dictions optimistes qui se r√©v√©leraient pr√©matur√©es."
        },
        {
          title: "√âchecs et le√ßons des syst√®mes symboliques",
          content: "Les approches symboliques des ann√©es 60-80 tentaient de codifier toutes les r√®gles du langage. R√©sultat : des syst√®mes fragiles, incapables de g√©rer les exceptions, les n√©ologismes, ou les usages cr√©atifs. Chaque nouvelle r√®gle cr√©ait de nouveaux cas particuliers. Cette approche, bien que logique, s'est heurt√©e √† la nature fondamentalement statistique et √©volutive du langage. Ces √©checs ont enseign√© une le√ßon cruciale : le langage ne peut pas √™tre enti√®rement captur√© par des r√®gles explicites."
        },
        {
          title: "L'√©volution vers l'approche statistique",
          content: "Les ann√©es 90 marquent un tournant : plut√¥t que d'essayer de programmer toutes les r√®gles, pourquoi ne pas laisser les machines apprendre des patterns dans d'√©normes corpus de texte ? Cette r√©volution statistique, puis neuronale, a ouvert la voie aux succ√®s actuels du NLP. Les machines ont commenc√© √† 'sentir' le langage plut√¥t qu'√† le diss√©quer. Cette approche empirique, bas√©e sur les donn√©es, s'est r√©v√©l√©e beaucoup plus robuste et adaptable."
        }
      ]
    },
    {
      icon: <History className="h-5 w-5 text-primary" />,
      title: "Approches historiques",
      items: [
        "Syst√®mes √† base de r√®gles grammaticales",
        "Dictionnaires et ontologies",
        "Analyse syntaxique et s√©mantique",
        "Limitations des approches symboliques",
        "Transition vers l'apprentissage automatique",
        "Naissance du NLP moderne"
      ],
      expandedExplanations: [
        {
          title: "Syst√®mes √† base de r√®gles grammaticales",
          content: "Les premiers syst√®mes NLP tentaient de reproduire l'analyse grammaticale humaine. Ils utilisaient des grammaires formelles, des arbres syntaxiques, et des r√®gles de transformation. Chaque langue n√©cessitait des ann√©es de travail linguistique pour √©tablir toutes les r√®gles. Ces syst√®mes √©taient pr√©cis sur les cas qu'ils connaissaient, mais cassants face √† l'inattendu. Un syst√®me pourrait parfaitement analyser 'Le chat mange la souris' mais √©chouer sur 'Le chat, affam√©, d√©vore goul√ªment sa proie'."
        },
        {
          title: "Dictionnaires et ontologies",
          content: "L'id√©e √©tait s√©duisante : cr√©er des dictionnaires exhaustifs reliant chaque mot √† son sens, ses synonymes, ses relations s√©mantiques. Des projets comme WordNet ont tent√© de cartographier tout le savoir humain en r√©seaux de concepts. Mais le sens d'un mot d√©pend tellement de son contexte que ces approches restaient limit√©es. Le mot 'banque' dans 'banque de donn√©es' n'a rien √† voir avec 'banque financi√®re', et encore moins avec 'banque de sable'."
        },
        {
          title: "Analyse syntaxique et s√©mantique",
          content: "Ces syst√®mes d√©composaient m√©ticuleusement chaque phrase : identification du sujet, verbe, compl√©ment, puis interpr√©tation du sens global. L'approche √©tait logique mais butait sur la complexit√© r√©elle du langage. Une phrase simple comme 'Time flies like an arrow' peut avoir plusieurs analyses syntaxiques valides : est-ce que le temps vole comme une fl√®che, ou bien les mouches temporelles aiment les fl√®ches ? Cette ambigu√Øt√© structurelle rendait l'analyse automatique extr√™mement difficile."
        },
        {
          title: "Limitations des approches symboliques",
          content: "Le principal probl√®me √©tait la rigidit√©. Ces syst√®mes ne savaient pas g√©rer l'ambigu√Øt√©, les fautes de frappe, les n√©ologismes, ou les usages cr√©atifs. Ils demandaient des ann√©es de d√©veloppement pour chaque langue et domaine. Pire, ils ne s'am√©lioraient pas avec l'usage, contrairement aux humains. Face √† un texte SMS avec des abr√©viations ou des emojis, ces syst√®mes √©taient compl√®tement perdus. Ils ne pouvaient pas non plus g√©rer l'√©volution naturelle de la langue."
        },
        {
          title: "Transition vers l'apprentissage automatique",
          content: "Les ann√©es 90 voient l'√©mergence d'approches statistiques. Au lieu de programmer des r√®gles, on laisse les algorithmes d√©couvrir des patterns dans de grandes quantit√©s de texte. Les mod√®les de n-grammes, puis les m√©thodes d'apprentissage supervis√©, commencent √† montrer des r√©sultats prometteurs sur des t√¢ches sp√©cifiques. Cette approche √©tait plus tol√©rante aux variations et pouvait s'am√©liorer automatiquement avec plus de donn√©es."
        },
        {
          title: "Naissance du NLP moderne",
          content: "L'av√®nement d'internet fournit soudain d'√©normes corpus de texte. Les algorithmes d'apprentissage automatique deviennent plus sophistiqu√©s. Les r√©seaux de neurones font leur retour apr√®s des d√©cennies d'hibernation. Cette convergence de donn√©es massives, d'algorithmes puissants et de capacit√© de calcul accrue donne naissance au NLP moderne que nous connaissons. Le machine learning remplace progressivement la programmation explicite de r√®gles."
        }
      ]
    },
    {
      icon: <Compass className="h-5 w-5 text-primary" />,
      title: "Objectifs modernes",
      items: [
        "Compr√©hension du sens et de l'intention",
        "G√©n√©ration de texte coh√©rent et cr√©atif",
        "Traduction fid√®le et nuanc√©e",
        "Interaction naturelle homme-machine",
        "Analyse de sentiment et d'√©motion",
        "R√©sum√© et synth√®se automatiques"
      ],
      expandedExplanations: [
        {
          title: "Compr√©hension du sens et de l'intention",
          content: "L'objectif ultime n'est plus seulement de reconna√Ætre des mots, mais de saisir ce que la personne veut vraiment dire. Cela implique de comprendre les intentions cach√©es, les √©motions sous-jacentes, et m√™me ce qui n'est pas dit explicitement. Les mod√®les modernes commencent √† d√©velopper une forme de 'th√©orie de l'esprit' artificielle, capable de comprendre que 'Il fait froid ici' peut √™tre une demande implicite de fermer la fen√™tre."
        },
        {
          title: "G√©n√©ration de texte coh√©rent et cr√©atif",
          content: "Aller au-del√† de la simple restitution d'informations pour cr√©er du contenu original, coh√©rent et adapt√© au contexte. Cela inclut l'√©criture cr√©ative, la g√©n√©ration de code, la cr√©ation de contenus marketing personnalis√©s, ou m√™me la composition po√©tique. L'IA devient un partenaire cr√©atif capable de maintenir un style, un ton, et une coh√©rence narrative sur de longs textes."
        },
        {
          title: "Traduction fid√®le et nuanc√©e",
          content: "D√©passer la traduction mot-√†-mot pour capturer les nuances culturelles, les jeux de mots, les r√©f√©rences implicites. Une bonne traduction moderne pr√©serve non seulement le sens, mais aussi le ton, le style et l'intention de l'auteur original. C'est un d√©fi qui touche √† l'essence m√™me de la communication interculturelle, n√©cessitant une compr√©hension profonde des deux cultures impliqu√©es."
        },
        {
          title: "Interaction naturelle homme-machine",
          content: "Cr√©er des interfaces o√π parler √† une machine se rapproche de parler √† un humain compr√©hensif. Cela implique de g√©rer les interruptions, les changements de sujet, les r√©f√©rences √† des conversations pr√©c√©dentes, et m√™me les silences significatifs. L'objectif est une fluidit√© conversationnelle naturelle o√π l'utilisateur oublie qu'il parle √† une machine."
        },
        {
          title: "Analyse de sentiment et d'√©motion",
          content: "Identifier non seulement ce qui est dit, mais comment c'est dit. D√©tecter la frustration dans un email de service client, l'enthousiasme dans un avis produit, ou la nostalgie dans un post sur les r√©seaux sociaux. Cette capacit√© ouvre des applications en psychologie, marketing, et relations humaines. Elle permet aux machines de r√©pondre de mani√®re empathique et appropri√©e au contexte √©motionnel."
        },
        {
          title: "R√©sum√© et synth√®se automatiques",
          content: "Face √† l'explosion informationnelle, les syst√®mes doivent savoir extraire l'essentiel de documents volumineux, synth√©tiser des points de vue multiples, et pr√©senter l'information de mani√®re structur√©e et accessible. C'est devenu crucial pour la gestion des connaissances et la prise de d√©cision. L'objectif est de cr√©er des r√©sum√©s qui capturent non seulement les faits, mais aussi les nuances et les implications."
        }
      ]
    }
  ];

  // Donn√©es pour les exemples de d√©fis et solutions
  const challengesData = {
    d√©fis: [
      "Ambigu√Øt√© lexicale et syntaxique",
      "R√©f√©rences contextuelles complexes", 
      "Expressions idiomatiques culturelles",
      "Nuances et sous-entendus",
      "√âvolution constante du langage"
    ],
    solutions: [
      "Approches statistiques sur corpus massifs",
      "Mod√®les neuronaux adaptatifs",
      "M√©canismes d'attention contextuelle", 
      "Apprentissage sur donn√©es diversifi√©es",
      "Architectures Transformer r√©volutionnaires"
    ]
  };

  // Exemples pratiques pour le pipeline NLP
  const nlpProcessSteps = [
    {
      title: "Tokenisation",
      description: "D√©coupage du texte en unit√©s traitables",
      input: "Bonjour le monde! Comment allez-vous?",
      output: "['Bonjour', 'le', 'monde', '!', 'Comment', 'allez', '-', 'vous', '?']",
      transformation: "Segmentation intelligente bas√©e sur les espaces, la ponctuation et les r√®gles linguistiques sp√©cifiques √† chaque langue",
      icon: <Scissors className="h-4 w-4" />,
      difficulty: "facile" as const
    },
    {
      title: "Normalisation",
      description: "Standardisation et nettoyage du texte",
      input: "['Bonjour', 'le', 'monde', '!', 'Comment', 'allez-vous', '?']",
      output: "['bonjour', 'le', 'monde', 'comment', 'allez-vous']",
      transformation: "Conversion en minuscules, suppression de la ponctuation, normalisation Unicode, gestion des caract√®res sp√©ciaux",
      icon: <Hash className="h-4 w-4" />,
      difficulty: "facile" as const
    },
    {
      title: "Vectorisation",
      description: "Conversion en repr√©sentations num√©riques",
      input: "['bonjour', 'le', 'monde', 'comment', 'allez-vous']",
      output: "[[0.2, -0.1, 0.8, ...], [0.1, 0.5, -0.3, ...], ...]",
      transformation: "Mapping vers des embeddings pr√©-entra√Æn√©s qui capturent la s√©mantique des mots dans un espace vectoriel dense",
      icon: <Calculator className="h-4 w-4" />,
      difficulty: "moyen" as const
    },
    {
      title: "Traitement contextuel",
      description: "Application du mod√®le NLP avanc√©",
      input: "S√©quence de vecteurs d'embeddings",
      output: "Repr√©sentations contextuelles enrichies",
      transformation: "R√©seau de neurones Transformer qui analyse les relations entre tous les mots simultan√©ment",
      icon: <Brain className="h-4 w-4" />,
      difficulty: "difficile" as const
    }
  ];

  // Concepts cl√©s enrichis
  const conceptsData = [
    {
      title: "Tokenisation intelligente",
      description: "Processus sophistiqu√© de d√©coupage du texte en unit√©s s√©mantiques optimales pour le traitement automatique.",
      example: "\"L'intelligence artificielle\" ‚Üí [\"L'\", \"intelligence\", \"art\", \"ificielle\"] (sous-mots)",
      difficulty: "D√©butant" as const,
      icon: <Scissors className="h-5 w-5" />,
      analogy: "Comme d√©couper une recette de cuisine en ingr√©dients individuels, mais en gardant ensemble ceux qui vont naturellement ensemble (comme 'pomme de terre').",
      technicalDetails: [
        "Gestion de la ponctuation et des espaces complexes",
        "Traitement des contractions et √©lisions (n'est, qu'il)",
        "Support multilingue et caract√®res Unicode vari√©s",
        "Algorithmes BPE et SentencePiece pour les sous-mots",
        "Optimisation pour le vocabulaire sp√©cifique du mod√®le"
      ],
      applications: ["Preprocessing", "Analyse syntaxique", "Recherche s√©mantique", "Traduction"]
    },
    {
      title: "Embeddings contextuels",
      description: "Repr√©sentations vectorielles dynamiques qui capturent le sens des mots selon leur contexte d'utilisation.",
      example: "'banque' ‚Üí [0.1, 0.8, -0.3] (rivi√®re) vs [0.7, -0.2, 0.5] (finance)",
      difficulty: "Interm√©diaire" as const,
      icon: <Network className="h-5 w-5" />,
      analogy: "Comme un cam√©l√©on qui change de couleur selon son environnement : le m√™me mot prend des 'couleurs' diff√©rentes selon le contexte.",
      technicalDetails: [
        "Dimensions typiques : 512-4096 pour les mod√®les modernes",
        "Apprentissage par co-occurrence contextuelle",
        "Propri√©t√©s d'analogie s√©mantique ('roi' - 'homme' + 'femme' ‚âà 'reine')",
        "Mise √† jour dynamique selon le contexte de la phrase",
        "Capture des relations syntaxiques et s√©mantiques complexes"
      ],
      applications: ["Similarit√© s√©mantique", "Analogies", "Classification", "Clustering", "Recherche"]
    },
    {
      title: "M√©canisme d'attention",
      description: "Innovation r√©volutionnaire permettant au mod√®le de se concentrer sur les parties pertinentes de l'entr√©e lors du traitement.",
      example: "Dans 'Le chat noir mange sa nourriture', 'mange' fait attention √† 'chat' et 'nourriture'",
      difficulty: "Avanc√©" as const,
      icon: <Eye className="h-5 w-5" />,
      analogy: "Comme votre ≈ìil qui balaie automatiquement une photo pour se concentrer sur les √©l√©ments importants selon ce que vous cherchez.",
      technicalDetails: [
        "Calcul des scores d'attention par produit scalaire",
        "Softmax pour la normalisation des poids",
        "Multi-head attention pour capturer diff√©rents types de relations",
        "Self-attention vs cross-attention pour diff√©rents usages",
        "Attention causale pour la g√©n√©ration s√©quentielle"
      ],
      applications: ["Traduction automatique", "R√©sum√© automatique", "Question-r√©ponse", "G√©n√©ration de texte"]
    },
    {
      title: "Architecture Transformer",
      description: "Architecture r√©volutionnaire bas√©e uniquement sur l'attention, abandonnant la r√©currence pour un traitement parall√®le.",
      example: "Encodeur-D√©codeur avec 12-96 couches d'attention multi-t√™tes",
      difficulty: "Avanc√©" as const,
      icon: <Layers className="h-5 w-5" />,
      analogy: "Comme un orchestre o√π chaque musicien √©coute simultan√©ment tous les autres pour jouer en harmonie parfaite.",
      technicalDetails: [
        "Architecture encodeur-d√©codeur modulaire",
        "Couches de self-attention et feed-forward",
        "Connexions r√©siduelles et normalisation",
        "Encodage positionnel pour l'ordre des mots",
        "Parall√©lisation massive du traitement"
      ],
      applications: ["LLM", "Traduction", "G√©n√©ration", "Compr√©hension", "Multimodal"]
    }
  ];

  const didYouKnowItems = [
    {
      title: "Ambigu√Øt√© linguistique record",
      content: "La phrase anglaise 'Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo' est grammaticalement correcte ! Elle signifie que les bisons de Buffalo (ville) que d'autres bisons de Buffalo intimident, intimident eux-m√™mes d'autres bisons de Buffalo. Cette phrase illustre parfaitement pourquoi le NLP est si complexe."
    },
    {
      title: "Vocabulaire gigantesque de GPT-4",
      content: "GPT-4 utilise un vocabulaire d'environ 100,000 tokens, permettant de repr√©senter efficacement la plupart des langues du monde. Chaque token peut repr√©senter un mot entier, une partie de mot, ou m√™me des caract√®res sp√©ciaux. Ce vocabulaire a √©t√© optimis√© pour √©quilibrer efficacit√© et couverture linguistique."
    },
    {
      title: "R√©volution Transformer",
      content: "L'architecture Transformer a r√©volutionn√© le NLP en 2017 avec un seul article : 'Attention is All You Need'. Aujourd'hui, pratiquement tous les grands mod√®les de langage (GPT, BERT, T5, PaLM, Claude) utilisent cette architecture ou ses variantes. C'est la plus grande r√©volution architecturale depuis les r√©seaux de neurones."
    },
    {
      title: "√âchelle astronomique des LLM",
      content: "GPT-3 a √©t√© entra√Æn√© sur environ 45 TB de texte, √©quivalent √† 45 millions de livres ou 22 millions d'articles Wikip√©dia. L'entra√Ænement a n√©cessit√© des milliers de GPU pendant plusieurs semaines, co√ªtant plusieurs millions de dollars. GPT-4 aurait co√ªt√© plus de 100 millions de dollars √† entra√Æner."
    },
    {
      title: "Capacit√©s √©mergentes myst√©rieuses",
      content: "Les grands mod√®les montrent des 'capacit√©s √©mergentes' : des aptitudes qui apparaissent soudainement au-del√† d'une certaine taille, comme le raisonnement math√©matique ou la programmation, sans avoir √©t√© explicitement entra√Æn√©s pour ces t√¢ches. C'est l'un des ph√©nom√®nes les plus fascinants et myst√©rieux de l'IA moderne."
    },
    {
      title: "Polyglotte num√©rique",
      content: "Les LLM modernes peuvent traduire entre plus de 100 langues, m√™me pour des paires de langues qu'ils n'ont jamais vues ensemble pendant l'entra√Ænement, gr√¢ce √† leur repr√©sentation interne commune des concepts. Ils d√©veloppent spontan√©ment une sorte de 'langue universelle' interne."
    }
  ];

  return (
    <>
      <Hero
        title={headerProps.title}
        subtitle={headerProps.subtitle}
      />

      <section className="section-container">
        <CourseHeader {...headerProps} />

        {/* Introduction enrichie et modernis√©e */}
        <CourseModule
          title="Introduction : L'odyss√©e de la compr√©hension artificielle"
          description="Embarquez dans un voyage fascinant √† travers l'√©volution du traitement du langage naturel"
        >
          <Card className="bg-gradient-to-br from-blue-50 via-white to-purple-50 dark:from-blue-950/20 dark:via-background dark:to-purple-950/20 border-primary/20">
            <CardContent className="pt-6">
              <div className="space-y-8">
                <div className="text-center">
                  <h3 className="text-2xl font-bold mb-4 text-foreground">üöÄ Bienvenue dans l'univers du NLP</h3>
                  <p className="text-lg leading-relaxed text-foreground max-w-4xl mx-auto">
                    Imaginez un monde o√π vous pourriez discuter avec votre ordinateur comme avec votre meilleur ami, 
                    lui demander d'analyser des milliers de documents en quelques secondes, de traduire 
                    instantan√©ment n'importe quel texte, ou encore de cr√©er des histoires captivantes sur commande. 
                    <strong className="text-primary"> Ce monde n'est plus de la science-fiction : c'est notre r√©alit√© d'aujourd'hui</strong>, 
                    rendue possible par les avanc√©es spectaculaires du traitement du langage naturel.
                  </p>
                </div>
                
                <div className="grid grid-cols-1 lg:grid-cols-2 gap-8">
                  <Card className="bg-gradient-to-br from-red-50 to-orange-50 dark:from-red-950/30 dark:to-orange-950/30 border-red-200 dark:border-red-800">
                    <CardHeader>
                      <CardTitle className="flex items-center gap-2 text-red-900 dark:text-red-100">
                        <Brain className="h-5 w-5" />
                        Le d√©fi titanesque
                      </CardTitle>
                    </CardHeader>
                    <CardContent>
                      <p className="text-red-800 dark:text-red-200">
                        Le langage humain est l'une des capacit√©s les plus complexes de notre esp√®ce. 
                        Nous jonglons naturellement avec l'ambigu√Øt√©, les m√©taphores, l'ironie, 
                        les r√©f√©rences culturelles... Enseigner cette ma√Ætrise √† une machine repr√©sente 
                        l'un des d√©fis les plus fascinants de l'intelligence artificielle.
                      </p>
                    </CardContent>
                  </Card>

                  <Card className="bg-gradient-to-br from-green-50 to-blue-50 dark:from-green-950/30 dark:to-blue-950/30 border-green-200 dark:border-green-800">
                    <CardHeader>
                      <CardTitle className="flex items-center gap-2 text-green-900 dark:text-green-100">
                        <Rocket className="h-5 w-5" />
                        L'√©volution fulgurante
                      </CardTitle>
                    </CardHeader>
                    <CardContent>
                      <p className="text-green-800 dark:text-green-200">
                        En seulement quelques d√©cennies, nous sommes pass√©s de simples correcteurs 
                        orthographiques √† des IA capables de converser, cr√©er, analyser et comprendre 
                        le langage avec une sophistication qui rivalise parfois avec les humains.
                      </p>
                    </CardContent>
                  </Card>
                </div>

                <AnalogyBox 
                  title="Le langage : notre superpouvoir √©volutif üß¨"
                  content="Si les humains dominent la plan√®te, c'est en grande partie gr√¢ce au langage. Cette capacit√© unique nous permet de transmettre des id√©es complexes, de collaborer √† grande √©chelle, et d'accumuler les connaissances de g√©n√©ration en g√©n√©ration. Reproduire artificiellement cette capacit√©, c'est toucher √† l'essence m√™me de l'intelligence humaine."
                  variant="tip"
                />

                <Card className="bg-gradient-to-r from-purple-50 to-pink-50 dark:from-purple-950/20 dark:to-pink-950/20 border-purple-200 dark:border-purple-800">
                  <CardContent className="pt-6">
                    <h4 className="font-medium flex items-center gap-2 mb-4 text-purple-900 dark:text-purple-100">
                      <Lightbulb className="h-5 w-5" />
                      Votre parcours d'apprentissage
                    </h4>
                    <div className="grid grid-cols-1 md:grid-cols-2 gap-6 text-sm">
                      <div>
                        <h5 className="font-medium mb-3 text-purple-900 dark:text-purple-100">üéØ Ce que vous d√©couvrirez :</h5>
                        <ul className="space-y-2 text-purple-800 dark:text-purple-200">
                          <li className="flex items-start gap-2">
                            <span className="text-purple-600 dark:text-purple-400 mt-1">‚Ä¢</span>
                            Comment une machine "lit" et "comprend" un texte
                          </li>
                          <li className="flex items-start gap-2">
                            <span className="text-purple-600 dark:text-purple-400 mt-1">‚Ä¢</span>
                            Les √©tapes de transformation du langage en nombres
                          </li>
                          <li className="flex items-start gap-2">
                            <span className="text-purple-600 dark:text-purple-400 mt-1">‚Ä¢</span>
                            L'√©volution des correcteurs aux IA conversationnelles
                          </li>
                          <li className="flex items-start gap-2">
                            <span className="text-purple-600 dark:text-purple-400 mt-1">‚Ä¢</span>
                            Les secrets des mod√®les comme ChatGPT et Claude
                          </li>
                        </ul>
                      </div>
                      <div>
                        <h5 className="font-medium mb-3 text-purple-900 dark:text-purple-100">üöÄ Comp√©tences acquises :</h5>
                        <ul className="space-y-2 text-purple-800 dark:text-purple-200">
                          <li className="flex items-start gap-2">
                            <span className="text-purple-600 dark:text-purple-400 mt-1">‚Ä¢</span>
                            Comprendre les enjeux du NLP moderne
                          </li>
                          <li className="flex items-start gap-2">
                            <span className="text-purple-600 dark:text-purple-400 mt-1">‚Ä¢</span>
                            Ma√Ætriser les concepts cl√©s (tokens, embeddings, attention)
                          </li>
                          <li className="flex items-start gap-2">
                            <span className="text-purple-600 dark:text-purple-400 mt-1">‚Ä¢</span>
                            Saisir le fonctionnement des Transformers
                          </li>
                          <li className="flex items-start gap-2">
                            <span className="text-purple-600 dark:text-purple-400 mt-1">‚Ä¢</span>
                            Appr√©hender les capacit√©s et limites des LLM
                          </li>
                        </ul>
                      </div>
                    </div>
                  </CardContent>
                </Card>
              </div>
            </CardContent>
          </Card>
        </CourseModule>

        {/* Module 1 avec sections interactives */}
        <CourseModule
          title="Module 1 : Comprendre le d√©fi de la compr√©hension automatique"
          description="Plongez dans la complexit√© fascinante du langage humain et d√©couvrez pourquoi il est si difficile √† automatiser"
        >
          <div className="space-y-8">
            {module1Data.map((module, index) => (
              <InteractiveSection 
                key={index} 
                {...module} 
                color={index === 0 ? 'blue' : index === 1 ? 'green' : 'purple'}
              />
            ))}
          </div>
        </CourseModule>

        {/* Comparaison historique enrichie */}
        <div className="my-12">
          <HistoricalComparison />
        </div>

        <ZoomOn title="L'exemple r√©v√©lateur de la traduction automatique">
          <div className="space-y-6">
            <Card className="bg-gradient-to-r from-blue-50 to-cyan-50 dark:from-blue-950/30 dark:to-cyan-950/30 border-blue-200 dark:border-blue-800">
              <CardContent className="pt-6">
                <p className="text-blue-900 dark:text-blue-100 mb-4">
                  En 1954, lors de la premi√®re d√©monstration publique de traduction automatique, 
                  IBM traduit avec fiert√© 60 phrases du russe vers l'anglais. Les chercheurs pr√©disent 
                  alors que le probl√®me sera r√©solu "dans trois √† cinq ans au maximum"...
                </p>
                
                <div className="bg-red-100 dark:bg-red-950/30 p-4 rounded-lg border-l-4 border-red-400">
                  <h5 className="font-medium text-red-900 dark:text-red-100 mb-2">üí• Exemple d'√©chec historique :</h5>
                  <div className="space-y-2 text-sm">
                    <p className="text-red-800 dark:text-red-200">
                      <strong>Phrase originale (anglais) :</strong> "The spirit is willing, but the flesh is weak."
                    </p>
                    <p className="text-red-800 dark:text-red-200">
                      <strong>Traduction en russe puis retour en anglais :</strong> "The vodka is good, but the meat is rotten."
                    </p>
                  </div>
                </div>

                <p className="text-blue-900 dark:text-blue-100 mt-4">
                  Soixante-dix ans plus tard, nous avons DeepL et Google Translate qui traduisent 
                  instantan√©ment dans plus de 100 langues avec une qualit√© remarquable. Mais le chemin 
                  a √©t√© sem√© d'emb√ªches qui nous ont appris √©norm√©ment sur la nature du langage.
                </p>
              </CardContent>
            </Card>

            <div className="grid grid-cols-1 lg:grid-cols-2 gap-6">
              <InteractiveSection
                title="D√©fis identifi√©s"
                icon={<AlertTriangle className="h-5 w-5" />}
                items={challengesData.d√©fis}
                color="orange"
              />
              <InteractiveSection
                title="Solutions d√©velopp√©es"
                icon={<Lightbulb className="h-5 w-5" />}
                items={challengesData.solutions}
                color="green"
              />
            </div>
          </div>
        </ZoomOn>

        {/* Pipeline de traitement avec exemples pratiques */}
        <div className="my-12">
          <div className="text-center mb-8">
            <h3 className="text-2xl font-bold mb-4 text-foreground">Pipeline de traitement NLP moderne</h3>
            <p className="text-muted-foreground max-w-3xl mx-auto">
              D√©couvrez √©tape par √©tape comment un texte brut devient compr√©hensible pour une machine
            </p>
          </div>
          <div className="grid gap-6">
            {nlpProcessSteps.map((step, index) => (
              <PracticalExample key={index} {...step} />
            ))}
          </div>
        </div>

        {/* Module 2 : Concepts fondamentaux */}
        <CourseModule
          title="Module 2 : Les fondamentaux techniques du NLP"
          description="Ma√Ætrisez les concepts essentiels qui transforment le texte en donn√©es exploitables"
        >
          <div className="grid grid-cols-1 md:grid-cols-2 gap-6">
            {conceptsData.map((concept, index) => (
              <ConceptCard key={index} {...concept} />
            ))}
          </div>
        </CourseModule>

        <DidYouKnow items={didYouKnowItems.slice(0, 3)} />

        {/* Module 3 : R√©volution Transformer */}
        <CourseModule
          title="Module 3 : La r√©volution Transformer"
          description="Explorez l'architecture qui a transform√© le NLP et donn√© naissance aux LLM modernes"
        >
          <div className="space-y-8">
            <Card>
              <CardHeader>
                <CardTitle className="flex items-center gap-2">
                  <Zap className="h-5 w-5 text-primary" />
                  "Attention Is All You Need" : L'article qui a tout chang√©
                </CardTitle>
              </CardHeader>
              <CardContent>
                <div className="space-y-4">
                  <p className="text-lg text-foreground">
                    Le 12 juin 2017, une √©quipe de Google publie un article de 11 pages qui va 
                    r√©volutionner l'intelligence artificielle. Le titre, provocateur, annonce 
                    que l'attention est tout ce dont on a besoin. Cette d√©claration audacieuse 
                    s'av√©rera proph√©tique.
                  </p>
                  
                  <AnalogyBox 
                    title="L'attention : comme votre cerveau lit une phrase"
                    content="Quand vous lisez 'Le chat que j'ai vu hier dormait paisiblement', votre cerveau ne traite pas chaque mot isol√©ment. Il fait automatiquement des liens : 'dormait' se rapporte √† 'chat', 'hier' modifie 'vu', etc. Le m√©canisme d'attention reproduit exactement cette capacit√© naturelle."
                  />

                  <div className="grid grid-cols-1 md:grid-cols-2 gap-6">
                    <div className="bg-red-50 dark:bg-red-950/20 p-4 rounded-lg">
                      <h5 className="font-medium text-red-900 dark:text-red-100 mb-3">Avant les Transformers (RNN/LSTM)</h5>
                      <ul className="text-red-800 dark:text-red-200 space-y-2 text-sm">
                        <li>‚Ä¢ <strong>Traitement s√©quentiel</strong> : mot par mot, lentement</li>
                        <li>‚Ä¢ <strong>M√©moire limit√©e</strong> : oubli des √©l√©ments distants</li>
                        <li>‚Ä¢ <strong>Pas de parall√©lisation</strong> : impossible d'acc√©l√©rer</li>
                        <li>‚Ä¢ <strong>Gradient vanishing</strong> : difficult√©s d'apprentissage</li>
                      </ul>
                    </div>
                    <div className="bg-green-50 dark:bg-green-950/20 p-4 rounded-lg">
                      <h5 className="font-medium text-green-900 dark:text-green-100 mb-3">Avec les Transformers</h5>
                      <ul className="text-green-800 dark:text-green-200 space-y-2 text-sm">
                        <li>‚Ä¢ <strong>Traitement parall√®le</strong> : tous les mots simultan√©ment</li>
                        <li>‚Ä¢ <strong>Attention globale</strong> : chaque mot "voit" tous les autres</li>
                        <li>‚Ä¢ <strong>Scalabilit√©</strong> : plus de donn√©es = meilleures performances</li>
                        <li>‚Ä¢ <strong>Transfert learning</strong> : r√©utilisation sur d'autres t√¢ches</li>
                      </ul>
                    </div>
                  </div>
                </div>
              </CardContent>
            </Card>

            {/* Visualisateur de Transformer */}
            <TransformerVisualizer />
          </div>
        </CourseModule>

        <div className="my-12">
          <ArchitectureComparison />
        </div>

        {/* Module 4 : LLM avec showcase statistiques */}
        <CourseModule
          title="Module 4 : L'√®re des Large Language Models (LLM)"
          description="D√©couvrez comment l'√©chelle a r√©v√©l√© des capacit√©s √©mergentes extraordinaires"
        >
          <LLMStatsShowcase />
        </CourseModule>

        <DidYouKnow items={didYouKnowItems.slice(3)} />

        {/* Module 5 : Applications futures */}
        <CourseModule
          title="Module 5 : Applications r√©volutionnaires et horizon futur"
          description="Explorez l'impact transformateur du NLP sur notre soci√©t√© et ses d√©veloppements √† venir"
        >
          <FutureApplications />
        </CourseModule>

        {/* Conclusion enrichie */}
        <CourseConclusion
          title="Conclusion : Vous ma√Ætrisez maintenant les fondements du NLP moderne"
          description="R√©capitulatif de votre parcours et ouverture vers l'avenir"
          learningPoints={[
            "Comprendre les d√©fis fondamentaux du traitement du langage naturel",
            "Ma√Ætriser les concepts cl√©s : tokenisation, embeddings, attention, Transformers",
            "Saisir l'importance r√©volutionnaire de l'architecture Transformer",
            "Appr√©hender le fonctionnement et les capacit√©s √©mergentes des LLM",
            "Identifier les applications concr√®tes et les enjeux soci√©taux",
            "Conna√Ætre les horizons futurs et les d√©fis √† relever"
          ]}
          nextSteps={[
            "Approfondissez avec les techniques de prompt engineering avanc√©",
            "Explorez le fine-tuning de mod√®les pour des t√¢ches sp√©cifiques",
            "D√©couvrez les mod√®les multimodaux (GPT-4V, DALL-E, etc.)",
            "Initiez-vous au d√©veloppement d'applications NLP concr√®tes",
            "Participez aux communaut√©s de recherche et open source",
            "Suivez les derni√®res publications scientifiques du domaine"
          ]}
        />
      </section>
    </>
  );
};

export default NLPComprehension;
